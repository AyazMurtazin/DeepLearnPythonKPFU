{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyazMurtazin/DeepLearnPythonKPFU/blob/main/ML/%D0%9F%D1%80%D0%B0%D0%BA%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_%E2%84%961_%D0%A0%D0%B5%D0%B3%D1%80%D0%B5%D1%81%D1%81%D0%B8%D0%BE%D0%BD%D0%BD%D1%8B%D0%B5_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Практическая работа №1. Регрессионные модели**\n",
        "\n",
        "#### **Введение**\n",
        "Цель работы: изучение методов построения и анализа регрессионных моделей для прогнозирования целевой переменной. В рамках работы рассматриваются этапы подготовки данных, создания новых признаков, обучения моделей, их интерпретации и оценки качества. Особое внимание уделяется этическим и правовым аспектам использования моделей.\n",
        "\n",
        "Работа включает следующие задачи:\n",
        "1. Подготовка данных.\n",
        "2. Feature Engineering.\n",
        "3. Анализ корреляции и проверка гипотез.\n",
        "4. Обучение базовых моделей регрессии.\n",
        "5. Автоматизация пайплайнов.\n",
        "6. Оптимизация гиперпараметров.\n",
        "7. Интерпретация моделей.\n",
        "8. Этический и правовой анализ.\n",
        "9. Вычисление метрик качества.\n",
        "10. Анализ результатов и выбор модели.\n",
        "\n",
        "\n",
        "### **Задача 1: Подготовка данных**\n",
        "\n",
        "1. **Цель**: Подготовить данные для анализа и обучения моделей.\n",
        "2. **Описание**:\n",
        "   - Выберите датасет из надежных источников:\n",
        "     - Kaggle, Yahoo Finance, Hugging Face, UCI Machine Learning Repository, Google Dataset Search, Data.gov, World Bank Open Data, Eurostat, FiveThirtyEight, AWS Open Data Registry, Quandl, OpenML, Figshare, KDD Cup Archives, StatLib Datasets Archive, Reddit Datasets.\n",
        "   - Загрузите данные из файла (например, CSV) или API.\n",
        "   - Проведите предварительный анализ данных:\n",
        "     - Проверьте наличие пропущенных значений. При необходимости заполните их (средним, медианой, KNN).\n",
        "     - Определите выбросы и аномалии с помощью статистических методов (например, межквартильного размаха) или методов машинного обучения (например, Isolation Forest или DBSCAN).\n",
        "   - Преобразуйте категориальные признаки в числовые форматы:\n",
        "     - Примените One-Hot Encoding для номинальных переменных.\n",
        "     - Используйте Label Encoding для порядковых переменных.\n",
        "   - Масштабируйте числовые признаки (например, стандартизация или нормализация) для улучшения работы моделей.\n",
        "   - Разделите данные на обучающую и тестовую выборки (например, в соотношении 80/20). Для временных данных используйте временную кросс-валидацию.\n",
        "3. **Входные данные**:\n",
        "   - Набор данных с признаками и целевой переменной.\n",
        "4. **Выходные данные**:\n",
        "   - Обучающая и тестовая выборки, готовые для обучения моделей.\n",
        "\n",
        "### **Задача 2: Feature Engineering**\n",
        "\n",
        "1. **Цель**: Создать новые признаки для улучшения качества модели.\n",
        "2. **Описание**:\n",
        "   - Создайте взаимодействия между признаками (например, произведение двух числовых признаков).\n",
        "   - Генерируйте полиномиальные признаки (например, квадраты или кубы числовых переменных).\n",
        "   - Преобразуйте признаки с помощью логарифмов, экспонент или других функций для снижения асимметрии распределения.\n",
        "   - Для временных рядов:\n",
        "     - Добавьте оконные признаки (lag features).\n",
        "     - Проведите проверку стационарности (тест Дики-Фуллера).\n",
        "   - Примените метод главных компонент (PCA) для снижения размерности.\n",
        "   - Проверьте мультиколлинеарность между новыми признаками (например, через корреляционную матрицу или VIF).\n",
        "3. **Входные данные**:\n",
        "   - Исходные признаки.\n",
        "4. **Выходные данные**:\n",
        "   - Новый набор признаков.\n",
        "\n",
        "\n",
        "\n",
        "### **Задача 3: Анализ корреляции и проверка гипотез**\n",
        "\n",
        "1. **Цель**: Проанализировать взаимосвязи между признаками и целевой переменной, а также сгенерировать и проверить гипотезы.\n",
        "2. **Описание**:\n",
        "   - Постройте корреляционную матрицу для числовых признаков и визуализируйте её с помощью тепловой карты.\n",
        "   - Проверьте статистические гипотезы:\n",
        "     - Например, проверьте гипотезу о равенстве средних значений целевой переменной для различных групп (t-тест или ANOVA).\n",
        "     - Проверьте гипотезу о нормальности распределения признаков (тест Шапиро-Уилка или Колмогорова-Смирнова).\n",
        "   - Оцените зависимость между категориальными и числовыми признаками (например, тест Краскела-Уоллиса).\n",
        "3. **Входные данные**:\n",
        "   - Набор данных с признаками и целевой переменной.\n",
        "4. **Выходные данные**:\n",
        "   - Корреляционная матрица, результаты статистических тестов.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Задача 3.1: Отбор признаков для моделирования**\n",
        "\n",
        "1. **Цель**: Выбрать наиболее значимые признаки для улучшения производительности и интерпретируемости моделей.\n",
        "\n",
        "2. **Описание**:\n",
        "   - **Статистические тесты**:\n",
        "     - Для числовых признаков использовать корреляционный анализ (Пирсона, Спирмена).\n",
        "     - Для категориальных признаков применить:\n",
        "       - Тест хи-квадрат\n",
        "       - Тест Краскела-Уоллиса\n",
        "       - ANOVA для сравнения средних между группами\n",
        "   - **Методы отбора признаков**:\n",
        "     - Фильтрация на основе статистических метрик (например, взаимная информация)\n",
        "     - Методы обертывания (wrapper methods):\n",
        "       - Recursive Feature Elimination (RFE)\n",
        "       - Последовательный отбор признаков\n",
        "     - Встроенные методы (embedded methods):\n",
        "       - Lasso-регрессия\n",
        "       - Анализ важности признаков в деревьях решений и их ансамблях\n",
        "   - **Анализ мультиколлинеарности**:\n",
        "     - Использование VIF (Variance Inflation Factor) для выявления сильно коррелирующих признаков\n",
        "     - Удаление или объединение избыточных признаков\n",
        "\n",
        "3. **Входные данные**:\n",
        "   - Набор данных после этапа Feature Engineering\n",
        "\n",
        "4. **Выходные данные**:\n",
        "   - Отобранный набор признаков\n",
        "   - Статистическая значимость каждого признака\n",
        "   - Метрики корреляции и взаимосвязей\n",
        "\n",
        "\n",
        "\n",
        "### **Задача 4: Обучение базовых моделей регрессии**\n",
        "\n",
        "1. **Цель**: Обучить базовые модели регрессии для сравнения их производительности.\n",
        "2. **Описание**:\n",
        "   - Реализуйте линейную регрессию как простую модель для прогнозирования.\n",
        "   - Примените Lasso-регрессию для отбора наиболее важных признаков.\n",
        "   - Используйте Ridge-регрессию для снижения переобучения за счет штрафа на величину коэффициентов.\n",
        "   - Комбинируйте L1 и L2 регуляризацию с помощью ElasticNet.\n",
        "   - Для временных данных используйте модели, специфичные для временных рядов, такие как ARIMA или Prophet.\n",
        "3. **Входные данные**:\n",
        "   - Обучающая выборка (признаки и целевая переменная).\n",
        "4. **Выходные данные**:\n",
        "   - Обученные модели регрессии.\n",
        "\n",
        "\n",
        "### **Задача 5: Автоматизация пайплайнов**\n",
        "\n",
        "1. **Цель**: Автоматизировать процесс подготовки данных и обучения моделей.\n",
        "2. **Описание**:\n",
        "   - Создайте пайплайн для автоматизации этапов предобработки данных и обучения модели.\n",
        "   - Используйте инструменты управления экспериментами для логирования параметров и результатов.\n",
        "   - Логируйте обученные модели для последующего использования.\n",
        "3. **Входные данные**:\n",
        "   - Данные и параметры модели.\n",
        "4. **Выходные данные**:\n",
        "   - Автоматизированный пайплайн и логи экспериментов.\n",
        "\n",
        "\n",
        "### **Задача 6: Оптимизация гиперпараметров**\n",
        "\n",
        "1. **Цель**: Оптимизировать гиперпараметры моделей для повышения их производительности.\n",
        "2. **Описание**:\n",
        "   - Используйте перебор возможных значений гиперпараметров.\n",
        "   - Для больших пространств поиска примените случайный поиск или байесовскую оптимизацию.\n",
        "   - Избегайте переобучения на валидационных данных — используйте nested cross-validation.\n",
        "3. **Входные данные**:\n",
        "   - Обучающая выборка, диапазон значений гиперпараметров.\n",
        "4. **Выходные данные**:\n",
        "   - Оптимальные значения гиперпараметров.\n",
        "\n",
        "\n",
        "### **Задача 7: Интерпретация моделей**\n",
        "\n",
        "1. **Цель**: Объяснить предсказания модели для лучшего понимания её работы.\n",
        "2. **Описание**:\n",
        "   - Визуализируйте важность признаков для линейных моделей (например, используя коэффициенты регрессии).\n",
        "   - Для нелинейных моделей:\n",
        "     - Используйте LIME для локальных объяснений.\n",
        "     - Постройте Partial Dependence Plots (PDP) для анализа влияния отдельных признаков.\n",
        "   - Примените SHAP-значения для глобальной интерпретации сложных моделей.\n",
        "3. **Входные данные**:\n",
        "   - Обученная модель и тестовые данные.\n",
        "4. **Выходные данные**:\n",
        "   - Графики и метрики интерпретации.\n",
        "\n",
        "\n",
        "### **Задача 8: Вычисление метрик качества**\n",
        "\n",
        "1. **Цель**: Оценить качество моделей с помощью метрик регрессии.\n",
        "2. **Описание**:\n",
        "   - Вычислите следующие метрики:\n",
        "     - MAE (Mean Absolute Error): среднее абсолютное отклонение.\n",
        "     - MSE (Mean Squared Error): среднеквадратичная ошибка.\n",
        "     - RMSE (Root Mean Squared Error): корень из среднеквадратичной ошибки.\n",
        "     - R² (Coefficient of Determination): коэффициент детерминации.\n",
        "     - sMAPE (симметричный MAPE) и MASE (Mean Absolute Scaled Error) для временных рядов.\n",
        "     - Кастомные метрики, связанные с бизнес-целями (например, прибыль).\n",
        "   - Сравните метрики для всех моделей и определите лучшую.\n",
        "3. **Входные данные**:\n",
        "   - Фактические и предсказанные значения.\n",
        "4. **Выходные данные**:\n",
        "   - Значения метрик для каждой модели.\n",
        "\n",
        "### **Задача 9: Анализ результатов и выбор модели**\n",
        "\n",
        "1. **Цель**: Выбрать лучшую модель на основе проведенного анализа.\n",
        "2. **Описание**:\n",
        "   - Сравните метрики качества всех моделей (MAE, MSE, RMSE, R²).\n",
        "   - Проанализируйте результаты диагностики остатков (случайность, нормальность).\n",
        "   - Предложите дальнейшие шаги для улучшения моделей:\n",
        "     - Добавление новых признаков.\n",
        "     - Использование более сложных алгоритмов (например, градиентный бустинг).\n",
        "     - Устранение мультиколлинеарности и переобучения.\n",
        "   - Проверьте модель на внешних данных для оценки обобщающей способности.\n",
        "3. **Входные данные**:\n",
        "   - Результаты всех предыдущих задач.\n",
        "4. **Выходные данные**:\n",
        "   - Рекомендации по выбору модели и улучшению её производительности.\n",
        "\n",
        "\n",
        "\n",
        "### **Задача 10: Исследование различных видов вычисления градиента**\n",
        "1. **Цель**: Исследовать методы вычисления градиентов для оптимизации моделей регрессии.\n",
        "2. **Описание**:\n",
        "   - Реализуйте вычисление градиента для линейной регрессии:\n",
        "     - Используйте аналитический подход (ручное вычисление производных функции потерь).\n",
        "     - Реализуйте численный расчет градиента (например, метод конечных разностей).\n",
        "     - Примените автоматическое дифференцирование с использованием библиотек (`torch.autograd`, `jax.grad` или `autograd`).\n",
        "   - Примените градиентный спуск для минимизации функции потерь:\n",
        "     - Реализуйте базовый градиентный спуск.\n",
        "     - Попробуйте стохастический градиентный спуск (SGD) и мини-пакетный градиентный спуск.\n",
        "   - Сравните результаты с оптимизацией, выполненной с помощью библиотек (например, `scipy.optimize` или `TensorFlow`/`PyTorch`).\n",
        "   - Оцените влияние гиперпараметров (например, скорость обучения, размер батча) на сходимость градиентного спуска.\n",
        "   - Визуализируйте процесс оптимизации:\n",
        "     - Постройте график изменения функции потерь в зависимости от количества итераций.\n",
        "     - Покажите траекторию движения параметров модели в пространстве признаков.\n",
        "3. **Входные данные**:\n",
        "   - Обучающая выборка (признаки и целевая переменная).\n",
        "4. **Выходные данные**:\n",
        "   - Реализованные алгоритмы вычисления градиента.\n",
        "   - Графики сходимости градиентного спуска.\n",
        "   - Сравнение результатов ручной реализации с библиотечными методами.\n",
        "5. **Методология**:\n",
        "   - Для аналитического вычисления градиента используйте формулу частных производных функции потерь (например, MSE).\n",
        "   - Для численного вычисления градиента используйте формулу конечных разностей:\n",
        "     $$\n",
        "     \\frac{\\partial f(x)}{\\partial x} \\approx \\frac{f(x + h) - f(x)}{h}, \\quad h \\to 0\n",
        "     $$\n",
        "   - Для автоматического дифференцирования используйте библиотеки:\n",
        "     - `torch.autograd` (PyTorch).\n",
        "     - `jax.grad` (JAX).\n",
        "     - `autograd`.\n",
        "   - Реализуйте градиентный спуск в следующем виде:\n",
        "     $$\n",
        "     \\theta := \\theta - \\alpha \\nabla_\\theta J(\\theta)\n",
        "     $$\n",
        "     где $J(\\theta)$ — функция потерь, $\\alpha$ — скорость обучения.\n",
        "   - Сравните скорость сходимости и точность между различными методами оптимизации.\n",
        "6. **Пример кода**:\n",
        "   ```python\n",
        "   import numpy as np\n",
        "   import torch\n",
        "   from jax import grad\n",
        "\n",
        "   # Аналитическое вычисление градиента для MSE\n",
        "   def compute_gradient(X, y, theta):\n",
        "       m = len(y)\n",
        "       predictions = X.dot(theta)\n",
        "       errors = predictions - y\n",
        "       gradient = (1 / m) * X.T.dot(errors)\n",
        "       return gradient\n",
        "\n",
        "   # Численное вычисление градиента\n",
        "   def numerical_gradient(X, y, theta, epsilon=1e-5):\n",
        "       grad = np.zeros_like(theta)\n",
        "       for i in range(len(theta)):\n",
        "           theta_plus = theta.copy()\n",
        "           theta_minus = theta.copy()\n",
        "           theta_plus[i] += epsilon\n",
        "           theta_minus[i] -= epsilon\n",
        "           grad[i] = (mse_loss(X, y, theta_plus) - mse_loss(X, y, theta_minus)) / (2 * epsilon)\n",
        "       return grad\n",
        "\n",
        "   # Автоматическое дифференцирование с PyTorch\n",
        "   def autograd_pytorch(X, y, theta):\n",
        "       X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "       y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "       theta_tensor = torch.tensor(theta, dtype=torch.float32, requires_grad=True)\n",
        "       \n",
        "       predictions = X_tensor @ theta_tensor\n",
        "       loss = torch.mean((predictions - y_tensor) ** 2)\n",
        "       loss.backward()\n",
        "       return theta_tensor.grad.numpy()\n",
        "\n",
        "   # Автоматическое дифференцирование с JAX\n",
        "   def autograd_jax(X, y, theta):\n",
        "       def mse_loss(theta):\n",
        "           predictions = X @ theta\n",
        "           return np.mean((predictions - y) ** 2)\n",
        "       grad_func = grad(mse_loss)\n",
        "       return grad_func(theta)\n",
        "\n",
        "   # Функция потерь (MSE)\n",
        "   def mse_loss(X, y, theta):\n",
        "       predictions = X.dot(theta)\n",
        "       return np.mean((predictions - y) ** 2)\n",
        "\n",
        "   # Градиентный спуск\n",
        "   def gradient_descent(X, y, learning_rate=0.01, iterations=1000):\n",
        "       theta = np.zeros(X.shape[1])\n",
        "       losses = []\n",
        "       for _ in range(iterations):\n",
        "           gradient = compute_gradient(X, y, theta)\n",
        "           theta -= learning_rate * gradient\n",
        "           losses.append(mse_loss(X, y, theta))\n",
        "       return theta, losses\n",
        "   ```\n",
        "\n",
        "\n",
        "### **11. Рекомендации по отчету**\n",
        "\n",
        "#### **Структура отчета:**\n",
        "1. **Раздел данных**:\n",
        "   - Описание датасета: краткая информация о данных, их источнике и признаках.\n",
        "   - Визуализация результатов EDA (Exploratory Data Analysis):\n",
        "     - Распределение целевой переменной и основных признаков.\n",
        "     - Корреляционная матрица или тепловая карта.\n",
        "     - Выбросы, пропущенные значения и аномалии (с визуализацией, например, boxplot или scatter plot).\n",
        "\n",
        "2. **Методология**:\n",
        "   - Схема preprocessing pipeline:\n",
        "     - Этапы предобработки данных (например, заполнение пропусков, масштабирование, кодирование категориальных признаков).\n",
        "     - Использованные инструменты (например, `Pipeline` из `sklearn`).\n",
        "   - Обоснование выбора моделей:\n",
        "     - Почему выбраны конкретные модели (например, линейная регрессия, Lasso, Ridge, нелинейные модели).\n",
        "     - Какие гиперпараметры были оптимизированы.\n",
        "\n",
        "3. **Результаты**:\n",
        "   - Сравнительная таблица метрик:\n",
        "     - MAE, MSE, RMSE, R², sMAPE, MASE (или другие метрики, если применимо).\n",
        "     - Укажите лучшую модель на основе метрик.\n",
        "   - Интерпретация лучшей модели:\n",
        "     - Важность признаков (например, коэффициенты регрессии для линейных моделей, SHAP-значения для сложных моделей).\n",
        "     - Анализ ошибок модели (например, случаи завышения/занижения прогнозов).\n",
        "\n",
        "4. **Приложения**:\n",
        "   - Код preprocessing:\n",
        "     - Основные этапы подготовки данных (можно добавить в виде скриншотов или выдержек из кода).\n",
        "   - Дополнительные графики:\n",
        "     - Любые дополнительные визуализации, которые помогают лучше понять данные или результаты моделирования.\n",
        "\n",
        "\n",
        "### **12. Критерии оценки**\n",
        "\n",
        "#### **Балльная система:**\n",
        "1. **Подготовка данных (20%)**:\n",
        "   - Полнота анализа данных (EDA): проверка пропусков, выбросов, корреляций.\n",
        "   - Качество предобработки данных: заполнение пропусков, масштабирование, кодирование категориальных признаков.\n",
        "   - Разделение данных на обучающую и тестовую выборки.\n",
        "\n",
        "2. **Feature engineering (15%)**:\n",
        "   - Создание новых признаков (полиномиальные, взаимодействия, преобразования).\n",
        "   - Применение методов снижения размерности (например, PCA).\n",
        "   - Проверка мультиколлинеарности и удаление лишних признаков.\n",
        "\n",
        "3. **Анализ и гипотезы (15%)**:\n",
        "   - Построение корреляционной матрицы и её интерпретация.\n",
        "   - Проверка статистических гипотез (например, t-тест, ANOVA, тест Шапиро-Уилка).\n",
        "   - Логичность выводов на основе анализа данных.\n",
        "\n",
        "4. **Моделирование (20%)**:\n",
        "   - Обоснование выбора моделей.\n",
        "   - Качество обучения моделей (например, отсутствие переобучения).\n",
        "   - Оптимизация гиперпараметров (например, через Grid Search или Random Search).\n",
        "\n",
        "5. **Интерпретация (15%)**:\n",
        "   - Ясность объяснения результатов моделирования.\n",
        "   - Использование инструментов интерпретации (например, SHAP, PDP, LIME).\n",
        "   - Анализ ошибок модели и предложение способов их устранения.\n",
        "\n",
        "\n",
        "6. **Вычисление градиентов (10%)**:\n",
        "   - Реализация аналитического, численного и автоматического вычисления градиента.\n",
        "   - Корректность реализации градиентного спуска.\n",
        "   - Сравнение скорости и точности различных методов вычисления градиента.\n",
        "\n",
        "\n",
        "7. **Оформление (5%)**:\n",
        "   - Четкость структуры отчета.\n",
        "   - Наличие всех необходимых разделов (данные, методология, результаты, приложения).\n",
        "   - Качество визуализаций и оформления таблиц.\n",
        "\n"
      ],
      "metadata": {
        "id": "ONWT7qr0SuVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 📘 **Regression Models: Полный Анализ**\n",
        "\n",
        "# Загружаем необходимые библиотеки\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from scipy.stats import shapiro\n",
        "\n",
        "import shap\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LQPApWPW-Wb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 📌 1. Загрузка и первичная обработка данных\n",
        "\n",
        "\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "df = data.frame\n",
        "df.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "13F73ONr_Z2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 🧹 2. Очистка данных и удаление выбросов\n",
        "\n",
        "# Проверка на пропущенные значения\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Удаление выбросов методом IQR\n",
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "df_cleaned = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "print(f\"Размер до: {df.shape}, после: {df_cleaned.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-xFpJbtU_cjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 🧾 3. Масштабирование и разделение данных\n",
        "\n",
        "\n",
        "X = df_cleaned.drop(columns='MedHouseVal')\n",
        "y = df_cleaned['MedHouseVal']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RPbw-L25_eoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "### 🛠️ 4. Feature Engineering: полиномиальные признаки и PCA\n",
        "\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_poly = poly.fit_transform(X_train)\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_pca = pca.fit_transform(X_poly)\n",
        "print(f\"Форма после PCA: {X_pca.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e9xjPvaE_gex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 🔎 5. Корреляционный анализ и нормальность\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(pd.DataFrame(X, columns=data.feature_names).corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Корреляционная матрица\")\n",
        "plt.show()\n",
        "\n",
        "for col in data.feature_names:\n",
        "    stat, p = shapiro(df[col])\n",
        "    print(f\"{col}: p = {p:.4f} — {'нормальное распределение' if p > 0.05 else 'не нормальное'}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ARM9l5Rz_h7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 📈 6. Построение моделей регрессии\n",
        "\n",
        "\n",
        "models = {\n",
        "    \"Linear\": LinearRegression(),\n",
        "    \"Lasso\": Lasso(alpha=0.1),\n",
        "    \"Ridge\": Ridge(alpha=1.0),\n",
        "    \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "}\n",
        "\n",
        "fitted_models = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    fitted_models[name] = model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UWXcmOT6_jAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 🔄 7. Пайплайн автоматизации\n",
        "\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', Ridge())\n",
        "])\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6TVecLrI_kPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 🔍 8. Подбор гиперпараметров\n",
        "\n",
        "\n",
        "grid = GridSearchCV(Ridge(), param_grid={'alpha': [0.01, 0.1, 1, 10]}, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(f\"Оптимальный параметр: {grid.best_params_}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rxm3-BW2_lSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 💡 9. Интерпретация моделей (SHAP)\n",
        "\n",
        "\n",
        "explainer = shap.Explainer(fitted_models['Linear'].predict, X_train)\n",
        "shap_values = explainer(X_test)\n",
        "shap.summary_plot(shap_values, X_test)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mzdKZTXZ_mE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 📊 10. Метрики качества моделей\n",
        "\n",
        "def evaluate(model):\n",
        "    preds = model.predict(X_test)\n",
        "    return {\n",
        "        \"MAE\": mean_absolute_error(y_test, preds),\n",
        "        \"MSE\": mean_squared_error(y_test, preds),\n",
        "        \"RMSE\": np.sqrt(mean_squared_error(y_test, preds)),\n",
        "        \"R2\": r2_score(y_test, preds)\n",
        "    }\n",
        "\n",
        "for name, model in fitted_models.items():\n",
        "    print(f\"{name}: {evaluate(model)}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-pqTMNXI_nS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ✅ 11. Анализ результатов\n",
        "\n",
        "# Сравнение по R2 и RMSE\n",
        "results = pd.DataFrame({name: evaluate(model) for name, model in fitted_models.items()}).T\n",
        "results.sort_values(\"R2\", ascending=False)\n"
      ],
      "metadata": {
        "id": "lwxZbpRx_ocZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O5DvK95H-gng"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}