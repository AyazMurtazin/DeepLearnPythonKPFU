{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyazMurtazin/DeepLearnPythonKPFU/blob/main/ML/RegressionTask1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **Самостоятельная работа №1**\n"
      ],
      "metadata": {
        "id": "MzhpTpNVi1B7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1) **Изучите математические основы линейной регрессии (в случае одномерной задачи):**\n"
      ],
      "metadata": {
        "id": "D0wohVe4yU5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   - Выведите формулы коэффициентов регрессии аналитически через **МНК (метод наименьших квадратов)**.\n",
        "   "
      ],
      "metadata": {
        "id": "gdv_ph2gyXAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Линейная регрессия: аналитический вывод через МНК и метод максимального правдоподобия**\n",
        "\n",
        "Рассмотрим **одномерную линейную регрессию** (случай парной регрессии):  \n",
        "$$ y_i = w_0 + w_1 x_i + \\varepsilon_i, \\quad i = 1, \\dots, n $$\n",
        "где:\n",
        "- (**$ y_i $**) — зависимая переменная (целевая),\n",
        "- (**$ x_i $**) — независимая переменная (признак),\n",
        "- (**$ w_0 $**) (сдвиг) и (**$ w_1 $**) (угловой коэффициент) — параметры модели,\n",
        "- (**$ \\varepsilon_i $**) — случайная ошибка (шум).\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Метод наименьших квадратов (МНК)**\n",
        "Цель МНК — минимизировать сумму квадратов ошибок (остатков):  \n",
        "$$ S(w_0, w_1) = \\sum_{i=1}^n (y_i - w_0 - w_1 x_i)^2. $$\n",
        "\n",
        "### **Находим оптимальные коэффициенты**\n",
        "1. **Частные производные** по (**$ w_0 $**) и (**$ w_1 $**):  \n",
        "   $$\n",
        "   \\frac{\\partial S}{\\partial w_0} = -2 \\sum_{i=1}^n (y_i - w_0 - w_1 x_i),  \n",
        "   \\quad\n",
        "   \\frac{\\partial S}{\\partial w_1} = -2 \\sum_{i=1}^n (y_i - w_0 - w_1 x_i) x_i.\n",
        "   $$\n",
        "\n",
        "2. **Приравниваем к нулю** и получаем систему уравнений:  \n",
        "   $$\n",
        "   \\begin{cases}\n",
        "   \\sum (y_i - w_0 - w_1 x_i) = 0, \\\\\n",
        "   \\sum (y_i - w_0 - w_1 x_i) x_i = 0.\n",
        "   \\end{cases}\n",
        "   $$\n",
        "\n",
        "3. **Решаем систему** (используем обозначения средних (**$ \\bar{x} = \\frac{1}{n}\\sum x_i $**), (**$ \\bar{y} = \\frac{1}{n}\\sum y_i $**)):  \n",
        "   $$\n",
        "   w_0 = \\bar{y} - w_1 \\bar{x},  \n",
        "   \\quad\n",
        "   w_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = \\frac{\\text{cov}(x, y)}{\\text{var}(x)}.\n",
        "   $$\n",
        "\n",
        "**Вывод:** МНК даёт явные формулы для коэффициентов, минимизирующих сумму квадратов ошибок.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Метод максимального правдоподобия (ММП)**\n",
        "Предположим, что ошибки (**$ \\varepsilon_i $**) **нормально распределены**:  \n",
        "$$ \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2). $$  \n",
        "Тогда (**$ y_i $**) также нормальны:  \n",
        "$$ y_i \\sim \\mathcal{N}(w_0 + w_1 x_i, \\sigma^2). $$\n",
        "\n",
        "### **Функция правдоподобия**\n",
        "$$\n",
        "L(w_0, w_1, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(y_i - w_0 - w_1 x_i)^2}{2 \\sigma^2} \\right).\n",
        "$$\n",
        "\n",
        "Логарифмируем (**логарифмическая функция правдоподобия**):  \n",
        "$$\n",
        "\\ln L = -\\frac{n}{2} \\ln(2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (y_i - w_0 - w_1 x_i)^2.\n",
        "$$\n",
        "\n",
        "### **Максимизация по (**$ w_0 $**) и (**$ w_1 $**)**\n",
        "Максимизация (**$ \\ln L $**) эквивалентна **минимизации суммы квадратов ошибок**:  \n",
        "$$\n",
        "\\sum (y_i - w_0 - w_1 x_i)^2 \\to \\min.\n",
        "$$\n",
        "\n",
        "Таким образом, оценки ММП для (**$ w_0 $**) и (**$ w_1 $**) **совпадают с МНК-оценками**.\n",
        "\n",
        "### **Оценка дисперсии ошибок**\n",
        "Максимизируя (**$ \\ln L $**) по (**$ \\sigma^2 $**), получаем:  \n",
        "$$\n",
        "\\hat{\\sigma}^2 = \\frac{1}{n} \\sum (y_i - \\hat{w}_0 - \\hat{w}_1 x_i)^2.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Связь МНК и максимального правдоподобия**\n",
        "- **Если ошибки нормальны**, то МНК-оценки совпадают с ММП-оценками.  \n",
        "- **Если ошибки не нормальны**, МНК даёт наилучшие линейные несмещённые оценки (BLUE), но ММП требует другого распределения ошибок.  \n",
        "\n",
        "**Итог:**  \n",
        "МНК — это частный случай ММП при нормальном распределении ошибок.  \n",
        "ММП более общий метод, но требует предположений о распределении."
      ],
      "metadata": {
        "id": "M9EmIeFRylrq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Модель линейной регрессии\n",
        "markdown\n",
        "$$ y_i = w_0 + w_1 x_i + \\varepsilon_i, \\quad i = 1, \\dots, n $$\n",
        "где:\n",
        "\n",
        "y\n",
        "i\n",
        "y\n",
        "i\n",
        "​\n",
        "  — целевая переменная,\n",
        "\n",
        "x\n",
        "i\n",
        "x\n",
        "i\n",
        "​\n",
        "  — признак,\n",
        "\n",
        "w\n",
        "0\n",
        ",\n",
        "w\n",
        "1\n",
        "w\n",
        "0\n",
        "​\n",
        " ,w\n",
        "1\n",
        "​\n",
        "  — параметры модели,\n",
        "\n",
        "ε\n",
        "i\n",
        "ε\n",
        "i\n",
        "​\n",
        "  — ошибка (шум).\n",
        "\n",
        "2. Метод наименьших квадратов (МНК)\n",
        "Минимизируем сумму квадратов ошибок:\n",
        "\n",
        "markdown\n",
        "$$ S(w_0, w_1) = \\sum_{i=1}^n (y_i - w_0 - w_1 x_i)^2 $$\n",
        "Оптимальные коэффициенты\n",
        "Частные производные:\n",
        "\n",
        "markdown\n",
        "$$\n",
        "\\frac{\\partial S}{\\partial w_0} = -2 \\sum_{i=1}^n (y_i - w_0 - w_1 x_i), \\quad\n",
        "\\frac{\\partial S}{\\partial w_1} = -2 \\sum_{i=1}^n (y_i - w_0 - w_1 x_i) x_i\n",
        "$$\n",
        "Решение системы:\n",
        "\n",
        "markdown\n",
        "$$\n",
        "w_0 = \\bar{y} - w_1 \\bar{x}, \\quad\n",
        "w_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2} = \\frac{\\text{cov}(x, y)}{\\text{var}(x)}\n",
        "$$\n",
        "3. Метод максимального правдоподобия (ММП)\n",
        "Предполагаем нормальность ошибок:\n",
        "\n",
        "markdown\n",
        "$$ \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2) $$\n",
        "Функция правдоподобия:\n",
        "\n",
        "markdown\n",
        "$$\n",
        "L(w_0, w_1, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\\frac{(y_i - w_0 - w_1 x_i)^2}{2 \\sigma^2} \\right)\n",
        "$$\n",
        "Логарифмическое правдоподобие:\n",
        "\n",
        "markdown\n",
        "$$\n",
        "\\ln L = -\\frac{n}{2} \\ln(2 \\pi \\sigma^2) - \\frac{1}{2 \\sigma^2} \\sum_{i=1}^n (y_i - w_0 - w_1 x_i)^2\n",
        "$$\n",
        "Оценка дисперсии:\n",
        "\n",
        "markdown\n",
        "$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{w}_0 - \\hat{w}_1 x_i)^2 $$\n",
        "4. Связь МНК и ММП\n",
        "markdown\n",
        "- Если ошибки нормальны (**$\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$**), оценки МНК и ММП **совпадают**.\n",
        "- МНК — это частный случай ММП для нормальных ошибок.\n",
        "- Для ненормальных ошибок МНК даёт BLUE-оценки, но ММП требует спецификации распределения."
      ],
      "metadata": {
        "id": "qR_I9XWEzNa6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - Проделайте вывод параметров модели с использованием **метода максимального правдоподобия**, предполагая нормальное распределение ошибок.\n"
      ],
      "metadata": {
        "id": "Mf23AeuGyaxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   - Объясните, как связаны **МНК** и **максимальное правдоподобие**.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H-_eB7yhyflb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) **Реализуйте с нуля класс `LinearRegression`, который поддерживает следующие функции:**\n",
        "   - Обучение модели методом **градиентного спуска**.\n",
        "   - Вычисление метрик качества:\n",
        "     - MSE (Mean Squared Error)\n",
        "     - MAE (Mean Absolute Error)\n",
        "     - R² (коэффициент детерминации)\n",
        "   - Механизм **early stopping** — остановка обучения при стабилизации ошибки на валидационной выборке.\n",
        "   - Визуализация:\n",
        "     - графика изменения ошибки от эпохи (**loss curve**),\n",
        "     - графика предсказаний модели против реальных значений,\n",
        "     - графика \"истинных точек vs предсказанных\".\n",
        "   - Возможность тестирования модели на новых данных (`predict`).\n",
        "\n",
        "\n",
        "\n",
        "## 3) Добавьте масштабирование признаков:\n",
        "   - Реализуйте собственную функцию стандартизации (Z-score), чтобы улучшить сходимость градиентного спуска.\n",
        "   - Сделайте масштабирование **опциональным аргументом** в `.fit()`.\n",
        "\n",
        "\n",
        "\n",
        "## 4) Добавьте возможность работы с многомерными данными:\n",
        "   - Расширьте реализацию для случая нескольких признаков (**множественная линейная регрессия**).\n",
        "   - Убедитесь, что модель корректно работает как с одним, так и с несколькими факторами.\n",
        "\n",
        "\n",
        "\n",
        "## 5) Протестируйте модель на различных наборах данных:\n",
        "   - Создайте **синтетический датасет** с помощью `numpy`.\n",
        "   - Проверьте работу модели на **реальных данных** (например, из библиотеки `sklearn.datasets` или Kaggle).\n",
        "   - Протестируйте модель при разных уровнях шума, выбросов и количестве признаков.\n",
        "\n",
        "\n",
        "## 6) Сравните вашу реализацию с `LinearRegression` из `sklearn`:\n",
        "   - Сравните качество модели по метрикам (MSE, R² и т.д.).\n",
        "   - Оцените разницу в предсказаниях и скорости сходимости.\n",
        "   - Проанализируйте, насколько ваши графики обучения и визуализации соответствуют ожиданиям.\n",
        "\n",
        "\n",
        "\n",
        "## 7) (Опционально) Добавьте дополнительные возможности:\n",
        "   - Поддержку **L1-регуляризации (Lasso)** и **L2-регуляризации (Ridge)**.\n",
        "   - Визуализацию весов модели.\n",
        "   - Логирование результатов обучения (например, сохранение метрик в CSV или JSON).\n",
        "   - Гиперпараметрический поиск (например, перебор learning rate, batch size и др.).\n",
        "\n",
        "\n",
        "\n",
        "# 📌 На память: Механизм Early Stopping (Ранняя остановка)\n",
        "\n",
        "### **Цель:**\n",
        "Предотвратить **переобучение модели** за счёт контроля качества на **валидационной выборке** во время обучения. Если модель перестаёт улучшаться, обучение останавливается.\n",
        "\n",
        "\n",
        "### **Как работает:**\n",
        "\n",
        "1. Данные делятся на:\n",
        "   - обучающую выборку (`X_train`, `y_train`)\n",
        "   - валидационную выборку (`X_val`, `y_val`)  \n",
        "     *(Если не передана — ранняя остановка не используется)*\n",
        "\n",
        "2. Обучение происходит итеративно (например, по эпохам в градиентном спуске).\n",
        "\n",
        "3. После каждой эпохи вычисляется значение **ошибки (loss)** на валидационной выборке.\n",
        "\n",
        "4. Если ошибка:\n",
        "   - **уменьшается** → продолжаем обучение и сохраняем текущую модель.\n",
        "   - **не уменьшается или растёт** → увеличиваем счётчик \"без улучшений\" (`counter`).\n",
        "     - Как только `counter >= patience`, обучение останавливается.\n",
        "\n",
        "### **Параметры:**\n",
        "- `patience` — количество эпох без улучшения, после которого обучение останавливается.\n",
        "- `min_delta` — минимальное улучшение ошибки, которое считается значимым (используется для фильтрации шума).\n",
        "\n",
        "\n",
        "### **Пример логики:**\n",
        "\n",
        "```python\n",
        "best_loss = float('inf')\n",
        "counter = 0\n",
        "for epoch in range(max_epochs):\n",
        "    train_one_epoch()            # Обучение на обучающих данных\n",
        "    val_loss = evaluate_on_val() # Вычисление ошибки на валидации\n",
        "\n",
        "    if val_loss < best_loss - min_delta:\n",
        "        best_loss = val_loss\n",
        "        counter = 0              # Сброс счётчика\n",
        "        save_model_weights()     # Сохраняем лучшие веса\n",
        "    else:\n",
        "        counter += 1             # Увеличиваем счётчик\n",
        "\n",
        "    if counter >= patience:\n",
        "        print(\"Early stopping!\")\n",
        "        break\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **Зачем это нужно?**\n",
        "- Предотвращает переобучение.\n",
        "- Ускоряет обучение — не нужно проходить все эпохи, если модель уже сошлась.\n",
        "- Автоматически выбирает оптимальное число эпох без ручного подбора.\n",
        "\n",
        "\n",
        "\n",
        "### **Пример сигнатуры метода `fit` с early stopping:**\n",
        "\n",
        "```python\n",
        "def fit(self, X_train, y_train, X_val=None, y_val=None,\n",
        "        epochs=1000, lr=0.01, batch_size=None,\n",
        "        early_stop=False, patience=5, min_delta=1e-4):\n",
        "```\n"
      ],
      "metadata": {
        "id": "SwxFRgk9yhHp"
      }
    }
  ]
}